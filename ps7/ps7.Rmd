---
title: "ps7"
author: "Ramon Crespo"
date: "11/15/2018"
output: pdf_document
---
Notes:
Worked on this problem by myself. 

Problem 1
Suppose I have a statistical method that estimates a regression coefficient and its standard error. I develop a simulation study and have m = 1000 simulated datasets that each give me an estimate of the coefficent and its standard error. How would I determine if the statistical method properly characterizes the uncertainty of the estimated regression coefficient?

A key element in determining the validity of the regression coeficients is to check for bias results. In the context of linear regression, this would be manifested in the linear regression model fitting a curve that is center slightly of the expected mean. 

```{r}
devs <- rnorm(100)
tdevs <- qt(pnorm(devs), df = 1) 
plot(devs)
```


Problem 2
Suppose I have a very large dataset, with n = 1e^9, so I have n observations and an n × p matrix X of predictors, where p = 8.
(a) Ordinarily, how much memory would the dataset take up?
Complete matrix = n = 1e9, d=8 --> 8e^9 numbers * 8 bytes per number = 64 GB

(b) Now suppose that there are only 10000 unique combinations of the p covariates. Given what you know about data structures in R, how could you store the data to use up much less memory? How much memory would be used by your solution?
You can store the 10000 unique combinations, 10000*8numbers*8bytes = 640,000 KB, and store an array of numbers that map the unique combinations to the values in the original matrix. The total space this takes is:
1e9*1*8bytes + 640,000KB = 8.64GB

(c) Now suppose you need to run lm(), glm(), etc. on this data. Why would all of your work in part (b) go to waste?
The functions would have to build back the matrix to be able to compute the calculations. In other words the original matrix will need to be recomputed for the lm() function to fit the linear model.


(d) If you need to find the OLS, (X.T*X)^-1*X.T*Y estimator here, how could you code this up (please provide pseudo-code; you don’t need to write any R code) so that you do not need to use up the full memory and can take advantage of your data structure(s).
Problem statement:
Performing calculations on datasets are specific to the structure and content of the dataset. For the purpose of this problem I will introduce some structure to the dataset so that my formulation makes sense. The context is the dataset consists of observations of the state of a system, and the entries in the dataset consist of how far the observations are from a reference state 0. You are mapping this observations to a rewar Y that is a function of the state d and some error e (Y is iid). The linear regression, would then be a predictor of rewards you can expect from visiting a specific state. 
In general, given that X = n x p matrix of and Y = p, it makes more sense to compute the operations in the following order = (X.T*X)^-1*(X.T*Y)

Pseudo code
-import matrix D and S where S is the matrix encoding the 10000 possible states and D is the map between S and the real data set R
-fit a model to the dataset D. 
  -Step1: obtain E[Y], E[(Y-E[Y])^2] for each possible state by using the observed rewards. This is computed using the reduced dataset containing the codes for the states visited, not the actual states.
  -Step2: Now that you have a distribution for the expected rewards in each state, you can sample acual states and rewards by using the distributions. Sample from the obtained distribution E[Y|D], E[(Y-E[Y])^2] by using rnorm(E[Y],E[(Y-E[Y])^2). Choose a proper size for the dataset, lets say 100000. 
  -Step3: compute w = (X.T*X)^-1*(X.T*Y) by using the sampled dataset. 
          -Step 3.1 = instead of solving the full problem above, use : QR decomposition. We could try to use Cholesky decomposition but we depend on X.T*X being pd which is highly unlikely because states are discrete thus some repetition is highly likely):
                -QR Decomposition
                      * X.T*X*w = X.T*Y
                      * R.T*Q.T*.Q*R*w = Q.T*Y
                      * R*w = Q.T*Y (this will be a backsolve since R is upper triangular)
                      
  -Step4: Check is results are consistent acros different simulations of the above procedure, specially making sure that you have not created a biased simulator that would lead to bias estimations of w.
  
The main point here is that in this context the actual data did not have to be accessed, and distributions could be obtained by just using the map (reduced dataset). Then expoit this structure to sample a smaller dataset and fit a linear regression to the reduced dataset, that is representative of the larger dataset. 

  
3. Suppose I need to compute the generalized least squares estimator, βˆ = (X.T*Σ^−1*X)^−1*X.T*Σ^−1*Y , for X n × p, Σ n × n and assume that n > p. Assume n could be of order several thousand and p of order in the hundreds. First write out in pseudo-code how you would do this in an efficient way - i.e., the particular linear algebra steps and the order of operations. Then write efficient R code in the form of a function, gls(), to do this - you can rely on the various high-level functions for matrix decompositions and solving systems of equations, but you should not use any code that already exists for doing generalized least squares.

Basic Idea:
1) Separate the equation into efficient matrix and vector operations
2) Avoid doing the naive inverse, solve a linear system of equations instead
3) Avoid explicitly doing the transpose of matrices, use code that computes that directly. 

pseudo-code
* compute the inverse of epsilon
* a - compute the crossproduct of X,epsilon
* a - compute the crossproduct of a,X
* a - compute the inverse of a
* b - compute the crossproduct of epsilon inverse and Y
* b - compute the crossproduct of X,b
* c - compute the crossproduct of a,b

```{r}
n <- 500
x <- matrix(rnorm(n*n,mean=100,sd=2),nrow=n,ncol=n)
y <- matrix(rnorm(n,mean=80,sd=1),nrow=n,ncol=1)
gls <- function(X,Y,epsilon) { 
  eps_inv = inv(epsilon)
  a = inv(crossprod(tcrossprod(X,epsilon),X))
  b = tcrosspro(X,crossprod(eps_inv,Y))
  c =crossprod(a,b)
  return(c)
}
```



4. We’ve seen how to use Gaussian elimination (i.e., the LU decomposition) to solve Ax=b and that we
can do the solution in n^3/3 operations (plus lower-order terms). Now let’s consider explicitly finding
A−1 and then calculating x = A−1*b via matrix-vector multiplication. If we look at R’s solve.default(),
we see it solves the system AZ = I to find Z = A−1 and help(solve) indicates it calls a Lapack routine
DGESV (http://www.netlib.org/lapack/explore-html/d7/d3b/group__double_g_esolve_ga5ee879032a8365897c3ba91e that uses the LU decomposition.
Count the number of computations for
(a) transforming AZ = I to UZ = I∗ (where I∗ is no longer a diagonal matrix),
This requires 2/3(n^3). This corresponds to transforming matriz A into an upper triangular matrix
(b) for solving for Z given UZ = I∗
O(n2)
(c) for calculating x = Zb.
O(n2)

total complexity  2/3(n^3)+2(n2)

5. Compare the speed of b = X−1y using: (a) solve(X)%*%y, (b) solve(X,y), and (c) Cholesky decomposition followed by solving triangular systems. Do this for a matrix of size 5000 × 5000 using a single thread, using a matrix X constructed from W ⊤ W where the elements of the n × n matrix W are generated independently using rnorm().


(a) How do the timing and relative ordering amongst methods compare to the order of computations we discussed in class, the notes, and above in problem 4.
The computational time are as follows:
* solve(X)%*%y = 189s
* solve(X,y) = 41s
* Cholesky Decomposition = 26s

The cholesky decomposition is the fasted as expected. From class notes an inclass work we know that LU O(n3) and Cholesky is also O(n3) but involves only half as many calculations. This follows the results where the Cholesky decomposition takes almost half of the time is takes solve(X,y). The naive way is summarized in solve(X)%*%y where the extra time and computational burdain comes from computing two separate calculations, one is solving the system of linear equations and solving for the identity matrix (solve(X)), and then using this solution times the right hand side of the linear system to obtain the result of the operations. From the previous questions we see that the computational complexity of the naive method is quite higher that the computational complexity of the more efficient methods, this follows the results in this part of the problem


(b) Are the results for b the same numerically for methods (b) and (c) (up to machine precision)? Comment on how many digits in the elements of b agree, and relate this to the condition number of the calculation.

The results are not the same for b and c. As seen in the code below there is a distance between the two solutions. This is a result of the rounding error in the computation of the matrices. The from the dataframe we conclude that the solutions are similar to the 5th decimal point. The condition number in the matrices make the solutions different, and specifically Cholesky decomposition is sensisitive to the values in the matrix.

```{r}
n <- 5000
w <- matrix(rnorm(n*n,mean=10,sd=2),nrow=n,ncol=n)
X <- t(w) %*% w
y <- matrix(rnorm(n,mean=100,sd=10),nrow=n,ncol=1)

time_a <-system.time(
a <- solve(X)%*%y
)

time_b <-system.time(
b <- solve(X,y)
)

time_c_1 <-system.time(
U <- chol(X)
)
time_c_2 <-system.time(
c_ <- backsolve(U, backsolve(U, y, transpose = TRUE))
)
print(time_a)
print(time_b)
print(time_c_1)
print(time_c_2)
```

```{r}
df <- data.frame(b,c_)
df[0:10,]
```

